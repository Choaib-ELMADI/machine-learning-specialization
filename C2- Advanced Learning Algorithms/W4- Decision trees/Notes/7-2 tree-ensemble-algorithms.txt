1. Tree Ensemble Algorithms:
    * Bagged Decision Tree:
        . This algorithm uses sampling with replacement (bagging) to create multiple slightly different training sets from the original data.
        . A decision tree is trained on each new dataset, and the results are combined (usually by voting) to make predictions.
        . It reduces variance and helps improve model robustness compared to a single decision tree.

    * Random Forest Algorithm: 
        . Builds on bagged decision trees by adding more randomness during training.
        . At each decision node, only a random subset of features is considered for splitting, rather than all features.
        . This increases diversity among the trees, leading to more robust and accurate predictions.
        . The typical number of trees in a random forest is around 100.

    * Boosted Decision Tree (XGBoost):
        . Builds on bagged decision trees by making it more likely to pick misclassified examples from previously trained trees (not from all examples).
        . This implementation enables the algorithm to focus more on the regions it's not doing well on yet.

        . XGBoost is a way of implementing boosting.
        . XGBoost stands for eXtreme Gradient Boosting.
        . Has built in regularization techniques to prevent overfitting.
        . It supports both classification and regression tasks with flexible library implementations:
            > from xgboost import XGBClassifier
            > from xgboost import XGBRegressor
