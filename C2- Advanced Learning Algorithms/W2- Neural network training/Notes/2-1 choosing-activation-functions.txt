1. Activation Function for Output Layer:
    * Based on the target label, there is one natural choice for the activation function:
        . For binary classification     --> Sigmoid activation function (gives the probability of being 1 or 0)
        . For multiclass classification --> Softmax activation function
        . For regression                --> Linear activation function
        . ...

2. Activation Function for Hidden Layers:
    * The ReLU activation function is by far the most common choice for hidden layers.
