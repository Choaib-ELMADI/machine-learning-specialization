1. What is Gradient Descent?
    * Gradient descent is an optimization algorithm used to minimize a function, such as a cost function in machine learning:
        . It systematically adjusts the parameters (w, b) to find the values that result in the smallest possible cost.
        . It is widely used across machine learning, including for training advanced models like "deep learning networks".

2. Implementing Gradient Descent:
    * Learning rate is a small positive value that determines the step size for updating the weights:
        . A smaller learning rate ===> slower learning
        . A larger one ===> can cause instability

    * The derivative of the cost function with respect to a parameter shows the direction of change of the cost:
        . The slope of a function at a point is the derivative of this function at this point.

    * Gradient descent takes smaller steps as it approaches a local minimum:
        . This happens because the derivative (slope) decreases near the minimum.
        . Smaller derivatives --> Smaller update steps, even if the learning rate is fixed.

3. Batch Gradient Descent:
    * Batch gradient descent is a variant of gradient descent where the algorithm computes updates by considering all the training examples at each step.
